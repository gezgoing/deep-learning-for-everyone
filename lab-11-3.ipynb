{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.409242731\n",
      "Epoch: 0002 cost = 0.085891434\n",
      "Epoch: 0003 cost = 0.065335395\n",
      "Epoch: 0004 cost = 0.053396829\n",
      "Epoch: 0005 cost = 0.047547864\n",
      "Epoch: 0006 cost = 0.044031710\n",
      "Epoch: 0007 cost = 0.038944595\n",
      "Epoch: 0008 cost = 0.036314502\n",
      "Epoch: 0009 cost = 0.033527773\n",
      "Epoch: 0010 cost = 0.030874100\n",
      "Epoch: 0011 cost = 0.029271407\n",
      "Epoch: 0012 cost = 0.027563748\n",
      "Epoch: 0013 cost = 0.027710342\n",
      "Epoch: 0014 cost = 0.026365754\n",
      "Epoch: 0015 cost = 0.024335015\n",
      "Learning Finished!\n",
      "Accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "        \n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "            learning_rate = 0.001\n",
    "\n",
    "            # L1 ImgIn shape=(?, 28, 28, 1)\n",
    "            W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "            #    Conv  shape=(?, 28, 28, 32)\n",
    "            #    Pool  shape=(?, 14, 14, 32)\n",
    "            L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L1 = tf.nn.relu(L1)\n",
    "            L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)\n",
    "\n",
    "            # L2 ImgIn shape=(?, 14, 14, 32)\n",
    "            W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "            #    Conv  shape=(?, 14, 14, 64)\n",
    "            #    Pool  shape=(?, 7, 7, 64)\n",
    "            L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L2 = tf.nn.relu(L2)\n",
    "            L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)\n",
    "\n",
    "            # L3 ImgIn shape=(?, 7, 7, 64)\n",
    "            W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "            #    Conv  shape=(?, 7, 7, 128)\n",
    "            #    Pool  shape=(?, 4, 4, 128)\n",
    "            L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L3 = tf.nn.relu(L3)\n",
    "            L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            L3 = tf.nn.dropout(L3, keep_prob=self.keep_prob)\n",
    "            L3 = tf.reshape(L3, [-1, 4 * 4 * 128])\n",
    "\n",
    "            with tf.variable_scope(\"W4\", reuse=tf.AUTO_REUSE) as scope:\n",
    "                W4 = tf.get_variable(\"W4\", shape=[4 * 4 * 128, 625], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b4 = tf.Variable(tf.random_normal([625]))\n",
    "            L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "            L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n",
    "\n",
    "            with tf.variable_scope(\"W5\", reuse=tf.AUTO_REUSE) as scope:\n",
    "                W5 = tf.get_variable(\"W5\", shape=[625, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b5 = tf.Variable(tf.random_normal([10]))\n",
    "            \n",
    "            self.hypothesis = tf.matmul(L4, W5) + b5\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.hypothesis, labels=self.Y))\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(self.hypothesis, 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.hypothesis, feed_dict={self.X: x_test, self.keep_prob: keep_prob})\n",
    "    \n",
    "    def get_accuracy(self, x_test, y_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.keep_prob: keep_prob})\n",
    "    \n",
    "    def train(self, x_data, y_data, keep_prob=0.7):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: x_data, self.Y: y_data, self.keep_prob: keep_prob})\n",
    "            \n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "m1 = Model(sess, \"m1\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "print('Learning started. It takes sometime.')\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        c, _ = m1.train(batch_xs, batch_ys)\n",
    "        \n",
    "        avg_cost += c / total_batch\n",
    "    \n",
    "    print('Epoch: %04d' % (epoch + 1), 'cost = {:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning Finished!')\n",
    "print('Accuracy:', m1.get_accuracy(mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "conv1 = tf.layers.conv2d(inputs=X_img, filter=32, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(input=conv1, pool_size=[2, 2], padding='SAME', strides=2)\n",
    "dropout1 = tf.layers.dropout(inputs=pool1, rate=self.keep_prob, training=self.training)\n",
    "\n",
    "...\n",
    "\n",
    "flat = tf.reshape(dropout3, [-1, 4 * 4 * 128])\n",
    "\n",
    "dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu)\n",
    "dropout = tf.layers.dropout(inputs=dense4, rate=self.keep_prob, training=self.training)\n",
    "\n",
    "```\n",
    "\n",
    "is equivalent for\n",
    "\n",
    "```python\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv  shape=(?, 28, 28, 32)\n",
    "#    Pool  shape=(?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)\n",
    "\n",
    "...\n",
    "\n",
    "L3 = tf.reshape(L3, [-1, 4 * 4 * 128])\n",
    "\n",
    "with tf.variable_scope(\"W4\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    W4 = tf.get_variable(\"W4\", shape=[4 * 4 * 128, 625], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost =  [0.42265444201501956, 0.4283716221221471, 0.4787348500943998, 0.5029855599694624, 0.4261229236627168, 0.4012170760976995, 0.3487490368871525]\n",
      "Epoch: 0002 cost =  [0.09800289445810705, 0.09407526589760723, 0.09156829074702483, 0.1023807594298639, 0.0907636711373926, 0.09897298017855408, 0.09718010613525453]\n",
      "Epoch: 0003 cost =  [0.06814085557718172, 0.07048237936994567, 0.06564465607516476, 0.07296031905338164, 0.06492001147686755, 0.07106103611009375, 0.0695070692321117]\n",
      "Epoch: 0004 cost =  [0.05872646928468544, 0.056847020964222846, 0.056605964043093045, 0.06271408763375473, 0.055970550934276095, 0.05875528884120287, 0.05852084967307747]\n",
      "Epoch: 0005 cost =  [0.053660374216058036, 0.048915701731616114, 0.048096786608750115, 0.0532807255620983, 0.049494725316551295, 0.051188840370794575, 0.05086150740781294]\n",
      "Epoch: 0006 cost =  [0.044180204580876646, 0.04544205811213365, 0.045520577177574684, 0.04808004963825539, 0.04179091994566, 0.04747820044402033, 0.045961586912585915]\n",
      "Epoch: 0007 cost =  [0.040472138341600905, 0.04305510469225491, 0.03996660433582629, 0.042023412589915106, 0.04018573629839176, 0.04258776831461793, 0.04206603879532351]\n",
      "Epoch: 0008 cost =  [0.038533526123078046, 0.03797382570041175, 0.03658994606209246, 0.04109669708223504, 0.03645886961921035, 0.040078899996482203, 0.038240617102604686]\n",
      "Epoch: 0009 cost =  [0.036008597622117534, 0.03589196915611282, 0.03389736512802881, 0.037314155746717, 0.031805778869893386, 0.033999253075506414, 0.03251984510930593]\n",
      "Epoch: 0010 cost =  [0.03378284667470411, 0.03270175760289104, 0.031234219661829127, 0.03461807493818922, 0.03277475134173236, 0.03457985147103586, 0.033355557777580216]\n",
      "Epoch: 0011 cost =  [0.031656522392824425, 0.029933544011278578, 0.02945065557797949, 0.03316948689446276, 0.03050271772647232, 0.03084988555638118, 0.032343843408479275]\n",
      "Epoch: 0012 cost =  [0.029771704846061786, 0.02829447424626611, 0.0285444335746367, 0.030655210874339745, 0.0295251510208684, 0.03194852979292839, 0.02981676060833378]\n",
      "Epoch: 0013 cost =  [0.029175266728852848, 0.028437472316224798, 0.029596913704140627, 0.02911082063801587, 0.027536221446088426, 0.025463079781409107, 0.029069335671797893]\n",
      "Epoch: 0014 cost =  [0.026502092211951756, 0.025913010832366787, 0.024331653246772455, 0.02725094116419894, 0.02619039331758605, 0.026890750445732397, 0.028314696028616936]\n",
      "Epoch: 0015 cost =  [0.027337295720935816, 0.02458361078561707, 0.025306867005261156, 0.026134706557548435, 0.025217685911497537, 0.02516241693631112, 0.025725024820845134]\n",
      "Learning Finished!\n",
      "0 Accuracy: 0.9937\n",
      "1 Accuracy: 0.9942\n",
      "2 Accuracy: 0.994\n",
      "3 Accuracy: 0.9934\n",
      "4 Accuracy: 0.9937\n",
      "5 Accuracy: 0.9949\n",
      "6 Accuracy: 0.9935\n",
      "Ensemble accuracy: 0.9951\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "models = []\n",
    "num_models = 7\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess, \"model\" + str(m)))\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "print('Learning started. It takes sometime.')\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = [0] * num_models\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        for index, model in enumerate(models):\n",
    "            c, _ = model.train(batch_xs, batch_ys)\n",
    "\n",
    "            avg_cost[index] += c / total_batch\n",
    "    \n",
    "    print('Epoch: %04d' % (epoch + 1), 'cost = ', avg_cost)\n",
    "    \n",
    "print('Learning Finished!')\n",
    "\n",
    "test_size = len(mnist.test.labels)\n",
    "predictions = np.zeros(test_size * 10).reshape(test_size, 10)\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    print(index, 'Accuracy:', model.get_accuracy(mnist.test.images, mnist.test.labels))\n",
    "    p = model.predict(mnist.test.images)\n",
    "    predictions += p\n",
    "    \n",
    "ensemble_correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
    "ensemble_accuracy = tf.reduce_mean(tf.cast(ensemble_correct_prediction, tf.float32))\n",
    "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
