{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU (with xavier initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from time import gmtime, strftime\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable('weight1', [784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable('bias1', [512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable('weight2', [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable('bias2', [512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable('weight3', [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.get_variable('bias3', [512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable('weight4', [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.get_variable('bias4', [512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable('weight5', [512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.get_variable('bias5', [10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "hypothesis = tf.nn.softmax(tf.matmul(layer4, W5) + b5)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, axis=1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 100\n",
    "num_iteartions = int(mnist.train.images.shape[0] / batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('cost', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter('./logs/lab-10/relu/%s' % (strftime(\"%Y_%b_%d_%H:%M:%S\", gmtime())))\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(num_iteartions):\n",
    "        batch_x_data, batch_y_data = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        s, cost_val, _ = sess.run([summary, cost, train], feed_dict={X: batch_x_data, Y: batch_y_data})\n",
    "\n",
    "        total_cost += cost_val\n",
    "        writer.add_summary(s, global_step=(epoch * num_iteartions + i))\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(total_cost / num_iteartions))\n",
    "\n",
    "acc_val = sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels})\n",
    "print(\"Accuracy: \", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU (with xavier initializer) + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhyuk/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.510115696\n",
      "Epoch: 0002 cost = 0.223968711\n",
      "Epoch: 0003 cost = 0.187260407\n",
      "Epoch: 0004 cost = 0.160886810\n",
      "Epoch: 0005 cost = 0.147330923\n",
      "Epoch: 0006 cost = 0.136313333\n",
      "Epoch: 0007 cost = 0.123007771\n",
      "Epoch: 0008 cost = 0.119440071\n",
      "Epoch: 0009 cost = 0.113938984\n",
      "Epoch: 0010 cost = 0.107471216\n",
      "Epoch: 0011 cost = 0.102643029\n",
      "Epoch: 0012 cost = 0.104564836\n",
      "Epoch: 0013 cost = 0.099155289\n",
      "Epoch: 0014 cost = 0.095349218\n",
      "Epoch: 0015 cost = 0.093514175\n",
      "Accuracy:  0.9812\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from time import gmtime, strftime\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable('weight1', [784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable('bias1', [512])\n",
    "_layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable('weight2', [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable('bias2', [512])\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable('weight3', [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.get_variable('bias3', [512])\n",
    "_layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "layer3 = tf.nn.dropout(_layer3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable('weight4', [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.get_variable('bias4', [512])\n",
    "_layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "layer4 = tf.nn.dropout(_layer4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable('weight5', [512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.get_variable('bias5', [10])\n",
    "hypothesis = tf.matmul(layer4, W5) + b5\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, axis=1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 100\n",
    "num_iteartions = int(mnist.train.images.shape[0] / batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('cost', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter('./logs/lab-10/relu/%s' % (strftime(\"%Y_%b_%d_%H:%M:%S\", gmtime())))\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(num_iteartions):\n",
    "        batch_x_data, batch_y_data = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        s, cost_val, _ = sess.run([summary, cost, train], feed_dict={X: batch_x_data, Y: batch_y_data, keep_prob: 0.5})\n",
    "\n",
    "        total_cost += cost_val\n",
    "        writer.add_summary(s, global_step=(epoch * num_iteartions + i))\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(total_cost / num_iteartions))\n",
    "\n",
    "acc_val = sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1})\n",
    "print(\"Accuracy: \", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.257181159\n",
      "Epoch: 0002 cost = 0.119386140\n",
      "Epoch: 0003 cost = 0.086130186\n",
      "Epoch: 0004 cost = 0.066374078\n",
      "Epoch: 0005 cost = 0.053661050\n",
      "Epoch: 0006 cost = 0.050136692\n",
      "Epoch: 0007 cost = 0.040774702\n",
      "Epoch: 0008 cost = 0.037753604\n",
      "Epoch: 0009 cost = 0.033226976\n",
      "Epoch: 0010 cost = 0.029461903\n",
      "Epoch: 0011 cost = 0.028500218\n",
      "Epoch: 0012 cost = 0.025268326\n",
      "Epoch: 0013 cost = 0.023799406\n",
      "Epoch: 0014 cost = 0.019142293\n",
      "Epoch: 0015 cost = 0.018783726\n",
      "Accuracy:  0.9798\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from time import gmtime, strftime\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "with tf.variable_scope('weight1', reuse=tf.AUTO_REUSE):\n",
    "    W1 = tf.get_variable('weight1', [784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('bias1', reuse=tf.AUTO_REUSE):\n",
    "    b1 = tf.get_variable('bias1', [512])\n",
    "\n",
    "layer1 = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.contrib.layers.batch_norm(layer1, center=True, scale=True, is_training=is_training)\n",
    "layer1 = tf.nn.relu(layer1)\n",
    "\n",
    "with tf.variable_scope('weight2', reuse=tf.AUTO_REUSE):\n",
    "    W2 = tf.get_variable('weight2', [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('bias2', reuse=tf.AUTO_REUSE):\n",
    "    b2 = tf.get_variable('bias2', [512])\n",
    "\n",
    "layer2 = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.contrib.layers.batch_norm(layer2, center=True, scale=True, is_training=is_training)\n",
    "layer2 = tf.nn.relu(layer2)\n",
    "\n",
    "with tf.variable_scope('weight3', reuse=tf.AUTO_REUSE):\n",
    "    W3 = tf.get_variable('weight3', [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('bias3', reuse=tf.AUTO_REUSE):\n",
    "    b3 = tf.get_variable('bias3', [512])\n",
    "\n",
    "layer3 = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.contrib.layers.batch_norm(layer3, center=True, scale=True, is_training=is_training)\n",
    "layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "with tf.variable_scope('weight4', reuse=tf.AUTO_REUSE):\n",
    "    W4 = tf.get_variable('weight4', [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('bias4', reuse=tf.AUTO_REUSE):\n",
    "    b4 = tf.get_variable('bias4', [512])\n",
    "\n",
    "layer4 = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.contrib.layers.batch_norm(layer4, center=True, scale=True, is_training=is_training)\n",
    "layer4 = tf.nn.relu(layer4)\n",
    "\n",
    "with tf.variable_scope('weight5', reuse=tf.AUTO_REUSE):\n",
    "    W5 = tf.get_variable('weight5', [512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('bias5', reuse=tf.AUTO_REUSE):\n",
    "    b5 = tf.get_variable('bias5', [10])\n",
    "\n",
    "layer5 = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.contrib.layers.batch_norm(layer5, center=True, scale=True, is_training=is_training)\n",
    "\n",
    "hypothesis = tf.nn.relu(layer5)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, axis=1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 100\n",
    "num_iteartions = int(mnist.train.images.shape[0] / batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('cost', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter('./logs/lab-10/batch_normalization/%s' % (strftime(\"%Y_%b_%d_%H:%M:%S\", gmtime())))\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(num_iteartions):\n",
    "        batch_x_data, batch_y_data = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        s, cost_val, _ = sess.run([summary, cost, train], feed_dict={X: batch_x_data, Y: batch_y_data, is_training: True})\n",
    "\n",
    "        total_cost += cost_val\n",
    "        writer.add_summary(s, global_step=(epoch * num_iteartions + i))\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(total_cost / num_iteartions))\n",
    "\n",
    "acc_val = sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, is_training: False})\n",
    "print(\"Accuracy: \", acc_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
